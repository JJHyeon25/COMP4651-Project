# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIDpuv-7dTGAVMOBKtoFCB0269s4QFfb

### 1. Initiate Spark environment
"""

!pip install pyspark

# Basic Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import random
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings('ignore')
from scipy import stats
from scipy.stats import norm, skew 

# Spark Library
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql import SQLContext, Window
from pyspark.sql.types import *
import pyspark.sql.functions as F
from pyspark.sql.functions import mean, col, split,regexp_extract, when, lit,max,min,isnan,count, desc,var_samp,avg, udf
from pyspark.ml.feature import StringIndexer, VectorAssembler,StandardScaler
# StringIndexer: mapping of string column of label to an ML column of label indice
# VectorAssembler: merge multiple column vector to single column
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import QuantileDiscretizer
from pyspark.ml.stat import Correlation
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.evaluation import RegressionEvaluator

spark_session = SparkSession.builder.master("local[2]").appName("HousingRegression").getOrCreate()

spark_context = spark_session.sparkContext

spark_sql_context = SQLContext(spark_context)

"""### 2. Loading data & EDA"""

# Commented out IPython magic to ensure Python compatibility.
 from google.colab import drive
 drive.mount("/content/drive")
#  %cd "drive/MyDrive/ColabNotebooks/comp4651_proj"

"""Use pandas to handle the missing data and create visualizations."""

pd_train = pd.read_csv('train.csv')
pd_test = pd.read_csv('test.csv')
na_cols = pd_train.columns[pd_train.isna().any()].tolist()

"""Building correlation matrix

The correlation coefficient is a measure of the strength and direction of the linear relationship between two variables.

The values in a correlation matrix range from -1 to 1. A value of 1 indicates a perfect positive correlation.  A value of -1 indicates a perfect negative correlation
"""

corr = pd_train.corr()

corr[['SalePrice']].sort_values(by='SalePrice',ascending=False).style.background_gradient(cmap='viridis', axis=None)

"""### Creating Spark DataFrames"""

train_df = spark_session.createDataFrame(pd_train)
test_df = spark_session.createDataFrame(pd_test)

"""### Handling missing data:
determining the proportion of missing data from overall dataset.
"""

total = pd_train.isnull().sum().sort_values(ascending=False)
percent = (pd_train.isnull().sum()/pd_train.shape[0]).sort_values(ascending=False)

missing = pd.concat([total, percent], axis=1, keys=['Total', 'Perc_missing'])
missing.head(15)

perc_df=train_df.select([(count(when(isnan(c), c))/count("Id")).alias(c) for c in train_df.columns])
perc_df.show()

drop_list=[ key for (key,value) in perc_df.collect()[0].asDict().items() if value > 0.15  ]
print(drop_list)

train_df=train_df.drop(*drop_list)
train_df.show()

"""### Changing Ordinal Categorical Data into Numerical Values"""

def encodeRating(df):
    df=df.withColumn("LotShape",when(col("LotShape")=="Reg",0).when(col("LotShape")=="IR1",1).when(col("LotShape")=="IR2",2).when(col("LotShape")=="IR3",3))
    df=df.withColumn("Utilities",when(col("Utilities")=="AllPub",3).when(col("Utilities")=="NoSewr",2).when(col("Utilities")=="NoSeWa",1).when(col("Utilities")=="ELO",0))
    df=df.withColumn("LandSlope",when(col("LandSlope")=="Gtl",0).when(col("LandSlope")=="Mod",1).when(col("LandSlope")=="Sev",2))
    df=df.withColumn("ExterQual",when(col("ExterQual")=="Ex",4).when(col("ExterQual")=="Gd",3).when(col("ExterQual")=="TA",2).when(col("ExterQual")=="Fa",1).when(col("ExterQual")=="Po",0))
    df=df.withColumn("ExterCond",when(col("ExterCond")=="Ex",4).when(col("ExterCond")=="Gd",3).when(col("ExterCond")=="TA",2).when(col("ExterCond")=="Fa",1).when(col("ExterCond")=="Po",0))
    df=df.withColumn("BsmtQual",when(col("BsmtQual")=="Ex",5).when(col("BsmtQual")=="Gd",4).when(col("BsmtQual")=="TA",3).when(col("BsmtQual")=="Fa",2).when(col("BsmtQual")=="Po",1).otherwise(0))
    df=df.withColumn("BsmtCond",when(col("BsmtCond")=="Ex",5).when(col("BsmtCond")=="Gd",4).when(col("BsmtCond")=="TA",3).when(col("BsmtCond")=="Fa",2).when(col("BsmtCond")=="Po",1).otherwise(0))
    df=df.withColumn("BsmtExposure",when(col("BsmtExposure")=="Gd",4).when(col("BsmtExposure")=="Av",3).when(col("BsmtExposure")=="Mn",2).when(col("BsmtExposure")=="No",1).otherwise(0))
    df=df.withColumn("BsmtFinType1",when(col("BsmtFinType1")=="GLQ",6).when(col("BsmtFinType1")=="ALQ",5).when(col("BsmtFinType1")=="BLQ",4).when(col("BsmtFinType1")=="Rec",3).when(col("BsmtFinType1")=="LwQ",2).when(col("BsmtFinType1")=="Unf",1).otherwise(0))
    df=df.withColumn("BsmtFinType2",when(col("BsmtFinType2")=="GLQ",6).when(col("BsmtFinType2")=="ALQ",5).when(col("BsmtFinType2")=="BLQ",4).when(col("BsmtFinType2")=="Rec",3).when(col("BsmtFinType2")=="LwQ",2).when(col("BsmtFinType2")=="Unf",1).otherwise(0))
    df=df.withColumn("HeatingQC",when(col("HeatingQC")=="Ex",4).when(col("HeatingQC")=="Gd",3).when(col("HeatingQC")=="TA",2).when(col("HeatingQC")=="Fa",1).otherwise(0))
    df=df.withColumn("CentralAir",when(col("CentralAir")=="N",0).when(col("CentralAir")=="Y",1))
    df=df.withColumn("Electrical",when(col("Electrical")=="SBrkr",4).when(col("Electrical")=="FuseA",3).when(col("Electrical")=="FuseF",2).when(col("Electrical")=="FuseP",1).otherwise(0))
    df=df.withColumn("KitchenQual",when(col("KitchenQual")=="Ex",4).when(col("KitchenQual")=="Gd",3).when(col("KitchenQual")=="TA",2).when(col("KitchenQual")=="Fa",1).otherwise(0))
    df=df.withColumn("Functional",when(col("Functional")=="Typ",6).when(col("Functional")=="Min1",6).when(col("Functional")=="Min2",5).when(col("Functional")=="Mod",4).when(col("Functional")=="Maj1",3).when(col("Functional")=="Maj2",2).when(col("Functional")=="Sev",1).when(col("Functional")=="Sal",0))
    df=df.withColumn("GarageFinish",when(col("GarageFinish")=="Fin",3).when(col("GarageFinish")=="RFn",2).when(col("GarageFinish")=="Unf",1).otherwise(0))
    df=df.withColumn("GarageQual",when(col("GarageQual")=="Ex",5).when(col("GarageQual")=="Gd",4).when(col("GarageQual")=="TA",3).when(col("GarageQual")=="Fa",2).when(col("GarageQual")=="Po",1).otherwise(0))
    df=df.withColumn("GarageCond",when(col("GarageCond")=="Ex",5).when(col("GarageCond")=="Gd",4).when(col("GarageCond")=="TA",3).when(col("GarageCond")=="Fa",2).when(col("GarageCond")=="Po",1).otherwise(0))
    df=df.withColumn("PavedDrive",when(col("PavedDrive")=="Y",2).when(col("PavedDrive")=="P",1).when(col("PavedDrive")=="N",0))
    df=df.fillna(0,['GarageYrBlt', 'GarageArea', 'GarageCars','MasVnrArea'])
    df=df.withColumn('MSSubClass',df['MSSubClass'].cast('string'))

    return df

def getAvg(df,feat):
    a=df.groupBy(feat).avg("SalePrice")
    # v=df.groupBy(feat).agg(var_samp("SalePrice"))
    # r=a.join(v,feat)
    return a

def encodeTarget(df,feat):
    avg_df=df.select([feat,"SalePrice"])
    avg_df=avg_df.groupBy(feat).mean("SalePrice").alias(feat+"_avg")
    avg_df=avg_df.select(F.col(feat),F.col("avg(SalePrice)").alias(feat+"_avg"))
    df=df.join(avg_df,feat)
    return df

train_df=encodeRating(train_df)
train_df.limit(5).show()
print(train_df.select([count(when(isnan(c), c)).alias(c) for c in train_df.columns]).show())

cat_feat=['MSSubClass','MSZoning','Street','LandContour','LotConfig',\
          'Neighborhood','Condition1','Condition2','BldgType','HouseStyle',\
          'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\
          'Foundation','Heating','GarageType','SaleType','SaleCondition']
numerical_feat=[x for x in train_df.columns if (x not in cat_feat)]

pd_train.columns

train_df.printSchema()

# import scipy.stats as stats

# asd=train_df.toPandas()
# catlen=[]
# for col in train_string_columns:
#     catlen.append(asd[col].unique().size)
# plt.figure(figsize=(18,16))

# for i,col in enumerate(train_string_columns):
#     unique_majors = asd[col].unique()
#     fig,ax = plt.subplots(nrows=1,ncols=catlen[i])
#     for j,major in enumerate(unique_majors):
#         stats.probplot(asd[asd[col] == major]['SalePrice'], dist="norm", plot=ax[j])
#         plt.title(major)

"""### Encode Categorical Data"""

# Defining string columns to pass on to the String Indexer (= categorical feature encoding)

train_string_columns = []

for col, dtype in train_df.dtypes:
    if dtype == 'string':
        train_string_columns.append(col)
print(train_string_columns)

# train_df.agg(*(F.countDistinct(col(c)).alias(c) for c in train_string_columns))
train_df.agg(*(F.countDistinct(F.col(c)).alias(c) for c in train_string_columns)).show()

train_df=encodeTarget(train_df,'Neighborhood')
train_df=encodeTarget(train_df,'Exterior1st')
train_df=encodeTarget(train_df,'Exterior2nd')

train_df.show(5)

from pyspark.ml import Pipeline
from pyspark.ml.feature import StringIndexer


indexers = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(train_df) for column in train_string_columns if column not in ['Neighborhood','Exterior1st','Exterior2nd'] ]


pipeline = Pipeline(stages=indexers)

train_indexed = pipeline.fit(train_df).transform(train_df)

train_df.show(5)

print(train_indexed.columns)

train_feat=[x for x in numerical_feat if (x!="Id" )]+[x for x in train_indexed.columns if "_" in x]

train_feat

# test_string_columns = []

# for col, dtype in test_df.dtypes:
#     if dtype == 'string':
#         test_string_columns.append(col)

# indexers2 = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(test_df) for column in test_string_columns]

# pipeline2 = Pipeline(stages=indexers2)
# test_indexed = pipeline2.fit(test_df).transform(test_df)

# print(len(test_indexed.columns))

# def get_dtype(df,colname):
#     return [dtype for name, dtype in df.dtypes if name == colname][0]

# num_cols_train = []
# for col in train_indexed.columns:
#     if get_dtype(train_indexed,col) != 'string':
#         num_cols_train.append(str(col))
        
# num_cols_test = []
# for col in test_indexed.columns:
#     if get_dtype(test_indexed,col) != 'string':
#         num_cols_test.append(str(col))

train_indexed = train_indexed.select(train_feat)
# test_indexed = test_indexed.select(num_cols_test)

print(set(train_feat).symmetric_difference(train_indexed.columns))
# print(len(test_indexed.columns))

# pd_train['New'] = pd_train['OverallQual'] * pd_train['GarageArea'] * pd_train['GrLivArea']
# pd_test['New'] = pd_test['OverallQual'] * pd_test['GarageArea'] * pd_test['GrLivArea']

# As some of the contestants have noticed, this results in a spike in model performance later

"""### 3. Model building (MLlib)"""

from pyspark.ml.feature import VectorAssembler
vectorAssembler = VectorAssembler(inputCols = train_indexed.drop("SalePrice").columns, outputCol = 'features').setHandleInvalid("keep")

train_vector = vectorAssembler.transform(train_indexed)

train_vector.show(5)

# vectorAssembler2 = VectorAssembler(inputCols = test_indexed.columns, outputCol = 'features').setHandleInvalid("keep")

# test_vector = vectorAssembler2.transform(test_indexed)

# from pyspark.sql.functions import lit

# test_vector = test_vector.withColumn("SalePrice", lit(0))

# Train-test split

splits = train_vector.randomSplit([0.7, 0.3])
train = splits[0]
val = splits[1]

# Full Feature

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol = 'features', labelCol='SalePrice', maxIter=10, 
                      regParam=0.8, elasticNetParam=0.1) # It is always a good idea to play with hyperparameters.
lr_model = lr.fit(train)

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

lr_predictions = lr_model.transform(val)
lr_predictions.select("prediction","SalePrice","features").show(5)

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="SalePrice",metricName="r2")
print("R Squared (R2) on val data = %g" % lr_evaluator.evaluate(lr_predictions))

top_feat=corr[['SalePrice']].sort_values(by='SalePrice',ascending=False).index.to_list()[1:14]+["Neighborhood_avg","Foundation_index","GarageType_index","MSSubClass_index","MasVnrType_index"]

vectorAssembler_top = VectorAssembler(inputCols = top_feat, outputCol = 'features_top').setHandleInvalid("keep")

train_vector_top = vectorAssembler_top.transform(train_indexed)

train_vector_top.show(1)

# Train-test split

splits_top = train_vector_top.randomSplit([0.7, 0.3])
train_top = splits_top[0]
val_top= splits_top[1]

# Simple baseline (linreg)

from pyspark.ml.regression import LinearRegression

lr = LinearRegression(featuresCol = 'features_top', labelCol='SalePrice', maxIter=10, 
                      regParam=0.8, elasticNetParam=0.1) # It is always a good idea to play with hyperparameters.
lr_model = lr.fit(train_top)

trainingSummary = lr_model.summary
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)

lr_predictions = lr_model.transform(val_top)
lr_predictions.select("prediction","SalePrice","features_top").show(5)

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="SalePrice",metricName="r2")
print("R Squared (R2) on val data = %g" % lr_evaluator.evaluate(lr_predictions))

train_feat

from pyspark.ml.regression import GBTRegressor

gbt=GBTRegressor(featuresCol='features',labelCol="SalePrice",maxIter=20,)
gbt_model=gbt.fit(train)

transform_df = gbt_model.transform(val)
evaluator = RegressionEvaluator(labelCol='SalePrice', metricName="r2")
print("R2: ", evaluator.evaluate(transform_df))

from itertools import chain

attrs = sorted(
    (attr["idx"], attr["name"])
    for attr in (
        chain(*train.schema["features"].metadata["ml_attr"]["attrs"].values())
    )
)

imp=dict(([
    (name, float(gbt_model.featureImportances[idx]))
    for idx, name in attrs
    if gbt_model.featureImportances[idx]
]))
sortedimp=dict(sorted(imp.items(), key=lambda item: item[1]))
sortedimp

"""### RANDOM FOREST PART"""

# A more complex model with RF

from pyspark.ml.regression import RandomForestRegressor

rf = RandomForestRegressor(featuresCol = 'features', labelCol='SalePrice', 
                           maxDepth=20, 
                           minInstancesPerNode=2,
                           bootstrap=True
                          )
rf_model = rf.fit(train)

rf_predictions = rf_model.transform(val)
rf_predictions.select("prediction","SalePrice","features").show(5)

from pyspark.ml.evaluation import RegressionEvaluator
rf_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="SalePrice",metricName="r2")
print("R Squared (R2) on val data = %g" % rf_evaluator.evaluate(rf_predictions))

from itertools import chain

attrs = sorted(
    (attr["idx"], attr["name"])
    for attr in (
        chain(*train.schema["features"].metadata["ml_attr"]["attrs"].values())
    )
)

imp=dict(([
    (name, float(rf_model.featureImportances[idx]))
    for idx, name in attrs
    if gbt_model.featureImportances[idx]
]))
sortedimp=dict(sorted(imp.items(), key=lambda item: item[1]))
sortedimp