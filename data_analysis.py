# -*- coding: utf-8 -*-
"""Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-cXsLqJqvI-K1IH0ytPgL5LPng0OA2_
"""

!pip install pyspark

"""# **VVVVVVVVV**

# Library Load & Data Load
"""

# Basic Library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import random
import os
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Spark Library
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql import SQLContext, Window
import pyspark.sql.functions as F
from pyspark.sql.functions import mean,col,split,regexp_extract, when, lit,max,min,isnan,count, desc,var_samp,avg
from pyspark.ml.feature import StringIndexer, VectorAssembler
# StringIndexer: mapping of string column of label to an ML column of label indice
# VectorAssembler: merge multiple column vector to single column
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import QuantileDiscretizer
from pyspark.ml.stat import Correlation
from pyspark.ml.regression import LinearRegression

import os
import sys

os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

spark_session = SparkSession.builder.master("local[2]").appName("HousingRegression").getOrCreate()

# Commented out IPython magic to ensure Python compatibility.
  from google.colab import drive
  drive.mount("/content/drive")
#   %cd "drive/My Drive/ColabNotebooks/comp4651_proj"
#data_dir='./'

# load the dataset and create spark dataframe
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
#train =  pd.read_csv(data_dir+'train.csv',index_col='Id')
#test = pd.read_csv(data_dir+'test.csv',index_col='Id')
train_df = spark_session.createDataFrame(train)
test_df = spark_session.createDataFrame(test)

# limit 5, like pandas head function
# toPandas make Pyspark DataFrame as Pandas table
train_df.limit(5).toPandas()

print(train_df.select([(count(when(isnan(c), c))/count("Id")).alias(c) for c in train_df.columns]).show())
# print(train_df.select("Fence").show())

train_df=train_df.drop('Alley').drop('PoolQC').drop('Fence').drop('MiscFeature')
train_df.limit(5).show()



def encodeRating(df):
    df=df.withColumn("LotShape",when(col("LotShape")=="Reg",0).when(col("LotShape")=="IR1",1).when(col("LotShape")=="IR2",2).when(col("LotShape")=="IR3",3))
    df=df.withColumn("Utilities",when(col("Utilities")=="AllPub",3).when(col("Utilities")=="NoSewr",2).when(col("Utilities")=="NoSeWa",1).when(col("Utilities")=="ELO",0))
    df=df.withColumn("LandSlope",when(col("LandSlope")=="Gtl",0).when(col("LandSlope")=="Mod",1).when(col("LandSlope")=="Sev",2))
    df=df.withColumn("ExterQual",when(col("ExterQual")=="Ex",4).when(col("ExterQual")=="Gd",3).when(col("ExterQual")=="TA",2).when(col("ExterQual")=="Fa",1).when(col("ExterQual")=="Po",0))
    df=df.withColumn("ExterCond",when(col("ExterCond")=="Ex",4).when(col("ExterCond")=="Gd",3).when(col("ExterCond")=="TA",2).when(col("ExterCond")=="Fa",1).when(col("ExterCond")=="Po",0))
    df=df.withColumn("BsmtQual",when(col("BsmtQual")=="Ex",5).when(col("BsmtQual")=="Gd",4).when(col("BsmtQual")=="TA",3).when(col("BsmtQual")=="Fa",2).when(col("BsmtQual")=="Po",1).otherwise(0))
    df=df.withColumn("BsmtCond",when(col("BsmtCond")=="Ex",5).when(col("BsmtCond")=="Gd",4).when(col("BsmtCond")=="TA",3).when(col("BsmtCond")=="Fa",2).when(col("BsmtCond")=="Po",1).otherwise(0))
    df=df.withColumn("BsmtExposure",when(col("BsmtExposure")=="Gd",4).when(col("BsmtExposure")=="Av",3).when(col("BsmtExposure")=="Mn",2).when(col("BsmtExposure")=="No",1).otherwise(0))
    df=df.withColumn("BsmtFinType1",when(col("BsmtFinType1")=="GLQ",6).when(col("BsmtFinType1")=="ALQ",5).when(col("BsmtFinType1")=="BLQ",4).when(col("BsmtFinType1")=="Rec",3).when(col("BsmtFinType1")=="LwQ",2).when(col("BsmtFinType1")=="Unf",1).otherwise(0))
    df=df.withColumn("BsmtFinType2",when(col("BsmtFinType2")=="GLQ",6).when(col("BsmtFinType2")=="ALQ",5).when(col("BsmtFinType2")=="BLQ",4).when(col("BsmtFinType2")=="Rec",3).when(col("BsmtFinType2")=="LwQ",2).when(col("BsmtFinType2")=="Unf",1).otherwise(0))
    df=df.withColumn("HeatingQC",when(col("HeatingQC")=="Ex",4).when(col("HeatingQC")=="Gd",3).when(col("HeatingQC")=="TA",2).when(col("HeatingQC")=="Fa",1).otherwise(0))
    df=df.withColumn("CentralAir",when(col("CentralAir")=="N",0).when(col("CentralAir")=="Y",1))
    df=df.withColumn("Electrical",when(col("Electrical")=="SBrkr",4).when(col("Electrical")=="FuseA",3).when(col("Electrical")=="FuseF",2).when(col("Electrical")=="FuseP",1).otherwise(0))
    df=df.withColumn("KitchenQual",when(col("KitchenQual")=="Ex",4).when(col("KitchenQual")=="Gd",3).when(col("KitchenQual")=="TA",2).when(col("KitchenQual")=="Fa",1).otherwise(0))
    df=df.withColumn("Functional",when(col("Functional")=="Typ",6).when(col("Functional")=="Min1",6).when(col("Functional")=="Min2",5).when(col("Functional")=="Mod",4).when(col("Functional")=="Maj1",3).when(col("Functional")=="Maj2",2).when(col("Functional")=="Sev",1).when(col("Functional")=="Sal",0))
    df=df.withColumn("FireplaceQu",when(col("FireplaceQu")=="Ex",5).when(col("FireplaceQu")=="Gd",4).when(col("FireplaceQu")=="TA",3).when(col("FireplaceQu")=="Fa",2).when(col("FireplaceQu")=="Po",1).otherwise(0))
    df=df.withColumn("GarageFinish",when(col("GarageFinish")=="Fin",3).when(col("GarageFinish")=="RFn",2).when(col("GarageFinish")=="Unf",1).otherwise(0))
    df=df.withColumn("GarageQual",when(col("GarageQual")=="Ex",5).when(col("GarageQual")=="Gd",4).when(col("GarageQual")=="TA",3).when(col("GarageQual")=="Fa",2).when(col("GarageQual")=="Po",1).otherwise(0))
    df=df.withColumn("GarageCond",when(col("GarageCond")=="Ex",5).when(col("GarageCond")=="Gd",4).when(col("GarageCond")=="TA",3).when(col("GarageCond")=="Fa",2).when(col("GarageCond")=="Po",1).otherwise(0))
    df=df.withColumn("PavedDrive",when(col("PavedDrive")=="Y",2).when(col("PavedDrive")=="P",1).when(col("PavedDrive")=="N",0))

    return df

def getAvg(df,feat):
    a=df.groupBy(feat).avg("SalePrice")
    # v=df.groupBy(feat).agg(var_samp("SalePrice"))
    # r=a.join(v,feat)
    return a

def encodeTarget(df,avg_df):
    df=df.join(avg_df,"MSSubClass")
    return df

cat_feat=['MSSubClass','MSZoning','Street','LandContour','LotConfig',\
          'Neighborhood','Condition1','Condition2','BldgType','HouseStyle',\
          'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType',\
          'Foundation','Heating','GarageType','SaleType','SaleCondition']
numerical_feat=[x for x in train_df.columns if (x not in cat_feat)]

train_df=encodeRating(train_df)
train_df.limit(5).show()
print(train_df.select([count(when(isnan(c), c)).alias(c) for c in train_df.columns]).show())

avg_MSSubClass=getAvg(train_df,"MSSubClass")
avg_MSZoning=getAvg(train_df,"MSZoning")
avg_Neighborhood=getAvg(train_df,"Neighborhood")
avg_MSSubClass.sort(desc("avg(SalePrice)")).show()
train_df=train_df.join(avg_MSSubClass,"MSSubClass").join(avg_MSZoning,"MSZoning").join(avg_Neighborhood,"Neighborhood")
train_df.limit(5).toPandas()

train_df.toPandas().boxplot(column=['SalePrice'],by=['MSSubClass'])
train_df.toPandas().boxplot(column=['SalePrice'],by=['Neighborhood'])
train_df.toPandas().boxplot(column=['SalePrice'],by=['MSZoning'])
train_df.toPandas().boxplot(column=['SalePrice'],by=['HouseStyle'])

"""#  **^^^^^^^^**"""

train_df = spark_session.createDataFrame(train)
train_df.describe().toPandas()

"""Data fields
Here's a brief version of what you'll find in the data description file.

- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
- MSSubClass: The building class
- MSZoning: The general zoning classification
- LotFrontage: Linear feet of street connected to property
- LotArea: Lot size in square feet
- Street: Type of road access
- Alley: Type of alley access
- LotShape: General shape of property
- LandContour: Flatness of the property
- Utilities: Type of utilities available
- LotConfig: Lot configuration
- LandSlope: Slope of property
- Neighborhood: Physical locations within Ames city limits
- Condition1: Proximity to main road or railroad
- Condition2: Proximity to main road or railroad (if a second is present)
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- OverallQual: Overall material and finish quality
- OverallCond: Overall condition rating
- YearBuilt: Original construction date
- YearRemodAdd: Remodel date
- RoofStyle: Type of roof
- RoofMatl: Roof material
- Exterior1st: Exterior covering on house
- Exterior2nd: Exterior covering on house (if more than one material)
- MasVnrType: Masonry veneer type
- MasVnrArea: Masonry veneer area in square feet
- ExterQual: Exterior material quality
- ExterCond: Present condition of the material on the exterior
- Foundation: Type of foundation
- BsmtQual: Height of the basement
- BsmtCond: General condition of the basement
- BsmtExposure: Walkout or garden level basement walls
- BsmtFinType1: Quality of basement finished area
- BsmtFinSF1: Type 1 finished square feet
- BsmtFinType2: Quality of second finished area (if present)
- BsmtFinSF2: Type 2 finished square feet
- BsmtUnfSF: Unfinished square feet of basement area
- TotalBsmtSF: Total square feet of basement area
- Heating: Type of heating
- HeatingQC: Heating quality and condition
- CentralAir: Central air conditioning
- Electrical: Electrical system
- 1stFlrSF: First Floor square feet
- 2ndFlrSF: Second floor square feet
- LowQualFinSF: Low quality finished square feet (all floors)
- GrLivArea: Above grade (ground) living area square feet
- BsmtFullBath: Basement full bathrooms
- BsmtHalfBath: Basement half bathrooms
- FullBath: Full bathrooms above grade
- HalfBath: Half baths above grade
- Bedroom: Number of bedrooms above basement level
- Kitchen: Number of kitchens
- KitchenQual: Kitchen quality
- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
- Functional: Home functionality rating
- Fireplaces: Number of fireplaces
- FireplaceQu: Fireplace quality
- GarageType: Garage location
- GarageYrBlt: Year garage was built
- GarageFinish: Interior finish of the garage
- GarageCars: Size of garage in car capacity
- GarageArea: Size of garage in square feet
- GarageQual: Garage quality
- GarageCond: Garage condition
- PavedDrive: Paved driveway
- WoodDeckSF: Wood deck area in square feet
- OpenPorchSF: Open porch area in square feet
- EnclosedPorch: Enclosed porch area in square feet
- 3SsnPorch: Three season porch area in square feet
- ScreenPorch: Screen porch area in square feet
- PoolArea: Pool area in square feet
- PoolQC: Pool quality
- Fence: Fence quality
- MiscFeature: Miscellaneous feature not covered in other categories
- MiscVal: Value of miscellaneous feature
- MoSold: Month Sold
- YrSold: Year Sold
- SaleType: Type of sale
- SaleCondition: Condition of sale
"""

# withColumn can be used to change the value of dataFrame
train_df = train_df.withColumn("SaleCondition",when(train_df.SaleCondition == "Normal",0).when(train_df.SaleCondition == "Abnorml",1))
train_df.limit(5).toPandas()

train_df.printSchema()

"""# EDA using PySpark"""

from pyspark.sql.types import StringType
# Check DataFrame's column dataType is string
cols = [f.name for f in train_df.schema.fields if isinstance(f.dataType, StringType)]

# Drop String type column
train_df.drop(*cols).printSchema()

# Detect Missing Value
columns = train_df.columns
missing_values = {}
for idx, column in enumerate(columns):
    missing_count = train_df.where(col(column).isNull()).count()
    missing_values.update({column:missing_count})
missing_df = pd.DataFrame.from_dict([missing_values])

missing_df

missing_df = pd.DataFrame(train_df.toPandas().isna().sum())
missing_df[missing_df[0]!=0]

traind_df = train_df.dropna()
train_df.printSchema()

train_df.select("SalePrice").toPandas().describe()

"""There is large difference between minimum and maximum values."""

train_df.select("SalePrice").toPandas().boxplot()

# Visualization SalePrice
plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=plt.figaspect(0.25))

sale_price = train_df.toPandas()[["SalePrice","Id"]]
sns.lineplot(y="SalePrice",x="Id",data=sale_price, ax=ax)

"""# **VVVVVVV**

## Rolling Average
"""

# train_df.toPandas().boxplot(column=['SalePrice'],by=['YrSold'])
avg_YearBuilt = train_df.select(['YearBuilt','SalePrice','OverallQual']).sort(F.asc("YearBuilt"))

avg_YearBuilt=avg_YearBuilt.withColumn("OverallQual",when(F.col("OverallQual")==10, 2).otherwise(F.floor((F.col("OverallQual")-1)/3)))
avg_YearBuilt.groupBy('OverallQual').count().show()
window = Window.orderBy("YearBuilt").partitionBy("OverallQual").rangeBetween(-5, Window.currentRow)
avg1=avg_YearBuilt.withColumn("value_array", F.collect_list(F.col("SalePrice")).over(window))\
    .withColumn("mean", F.expr('AGGREGATE(value_array, DOUBLE(0), (acc, x) -> acc + x, acc -> acc / size(value_array))'))
window2 = Window.orderBy("YearBuilt").rangeBetween(-5, Window.currentRow)
avg2=avg_YearBuilt.withColumn("value_array", F.collect_list(F.col("SalePrice")).over(window2))\
    .withColumn("mean", F.expr('AGGREGATE(value_array, DOUBLE(0), (acc, x) -> acc + x, acc -> acc / size(value_array))'))
# sns.lineplot(y="SalePrice",x="YearBuilt",data=avg_YrSold)

mv0=avg1.filter(F.col("OverallQual")==0).select(["YearBuilt","mean"]).toPandas()
mv1=avg1.filter(F.col("OverallQual")==1).select(["YearBuilt","mean"]).toPandas()
mv2=avg1.filter(F.col("OverallQual")==2).select(["YearBuilt","mean"]).toPandas()
mv3=avg2.select(["YearBuilt","mean"]).toPandas()
# mv4=avg_YearBuilt.filter(F.col("OverallQual")==5).select(["YearBuilt","mean"]).toPandas()

fig, ax = plt.subplots()
plt.title('Rolling Average Sale Price vs Build Year for every Overall Rating Group')
poor,=ax.plot('YearBuilt',"mean",data=mv0,color='r',label='1-3')
mid,=ax.plot('YearBuilt',"mean",data=mv1,color='#ffa500',label='4-6')
gd,=ax.plot('YearBuilt',"mean",data=mv2,color='#32cd32',label='7-10')
ttl,=ax.plot('YearBuilt',"mean",data=mv3,color='c',ls='--',label='combined')
# plt.plot('YearBuilt',"mean",data=mv4,color='#7cfc00',label='9-10')
ax.legend(handles=[poor, mid,gd,ttl])
plt.xlabel("Build Year")
plt.ylabel("Sale Price")
plt.show()

# train_df.toPandas().boxplot(column=['SalePrice'],by=['YrSold'])
avg_remodel = train_df.select(['YearRemodAdd','SalePrice','OverallQual']).sort(F.asc("YearRemodAdd"))

avg_remodel=avg_remodel.withColumn("OverallQual",when(F.col("OverallQual")==10, 2).otherwise(F.floor((F.col("OverallQual")-1)/3)))
avg_remodel.groupBy('OverallQual').count().show()
window = Window.orderBy("YearRemodAdd").partitionBy("OverallQual").rangeBetween(-5, Window.currentRow)
avg1=avg_remodel.withColumn("value_array", F.collect_list(F.col("SalePrice")).over(window))\
    .withColumn("mean", F.expr('AGGREGATE(value_array, DOUBLE(0), (acc, x) -> acc + x, acc -> acc / size(value_array))'))
window2 = Window.orderBy("YearRemodAdd").rangeBetween(-5, Window.currentRow)
avg2=avg_remodel.withColumn("value_array", F.collect_list(F.col("SalePrice")).over(window2))\
    .withColumn("mean", F.expr('AGGREGATE(value_array, DOUBLE(0), (acc, x) -> acc + x, acc -> acc / size(value_array))'))
# sns.lineplot(y="SalePrice",x="YearBuilt",data=avg_YrSold)

mv0=avg1.filter(F.col("OverallQual")==0).select(["YearRemodAdd","mean"]).toPandas()
mv1=avg1.filter(F.col("OverallQual")==1).select(["YearRemodAdd","mean"]).toPandas()
mv2=avg1.filter(F.col("OverallQual")==2).select(["YearRemodAdd","mean"]).toPandas()
mv3=avg2.select(["YearRemodAdd","mean"]).toPandas()
# mv4=avg_YearBuilt.filter(F.col("OverallQual")==5).select(["YearBuilt","mean"]).toPandas()

fig, ax = plt.subplots()
plt.title('Rolling Average Sale Price vs Remodel Year for every Overall Rating Group')
poor,=ax.plot('YearRemodAdd',"mean",data=mv0,color='r',label='1-3')
mid,=ax.plot('YearRemodAdd',"mean",data=mv1,color='#ffa500',label='4-6')
gd,=ax.plot('YearRemodAdd',"mean",data=mv2,color='#32cd32',label='7-10')
ttl,=ax.plot('YearRemodAdd',"mean",data=mv3,color='c',ls='--',label='combined')
# plt.plot('YearBuilt',"mean",data=mv4,color='#7cfc00',label='9-10')
ax.legend(handles=[poor, mid,gd,ttl])
plt.xlabel("Remodel Year")
plt.ylabel("Sale Price")
plt.show()

def corr_ratio(df,cat_feat,target):
    mean=df.select(avg(col(target))).first()[0]
    d=df.agg(F.sum((df.SalePrice-mean)**2)).first()[0]
    r={}
    for feat in cat_feat:
        a=df.groupBy(feat).count().join(df.groupBy(feat).avg(target),feat)
        result=a.agg(F.sum(col('count')*(col(f'avg({target})')-mean)**2)).first()[0]
        r[feat]=(result/d)**0.5
    # print(r)
    return r
cat_corr=corr_ratio(train_df,cat_feat,'SalePrice')

# [i,v for i,v in cat_corr.items()]
sns.heatmap(pd.DataFrame([v for i,v in cat_corr.items()]
,index=cat_feat,columns=['correlation']).sort_values(by=["correlation"]),annot=True)

num_corr=pd.DataFrame(train_df.select(numerical_feat).toPandas().corr(),columns=["SalePrice"]).sort_values(by='SalePrice')

plt.figure(figsize=(30,5))
sns.heatmap(num_corr.T,annot=True)

cat_corr_qual=corr_ratio(train_df,cat_feat,'OverallQual')

sns.heatmap(pd.DataFrame([v for i,v in cat_corr_qual.items()]
,index=cat_feat,columns=['correlation']).sort_values(by=["correlation"]),annot=True)

num_corr_qual=pd.DataFrame(train_df.select(numerical_feat).toPandas().corr(),columns=["OverallQual"]).sort_values(by='OverallQual')
plt.figure(figsize=(30,5))
sns.heatmap(num_corr_qual.T,annot=True)

set(list(num_corr_qual.index.values)[-15:-2]).symmetric_difference(list(num_corr.index.values)[-15:-2])

"""# **^^^^^**"""

train_df.filter(train_df.YearBuilt.isin([1880,1892,1893])).toPandas()[['YearBuilt','YearRemodAdd','SalePrice']].sort_values('SalePrice')

# Add bulit_remod column and append new data(between built and remodeling)
train_df = train_df.withColumn('Built_Remod', train_df.YearRemodAdd - train_df.YearBuilt)
train_df.toPandas()

# Checking the number of years passed before remodeling after the house has been built
train_df.groupBy("Built_Remod").count().orderBy("Built_Remod").toPandas()

train_df_remodeling_info = train_df.groupBy("Built_Remod").count().orderBy("Built_Remod").toPandas()
sns.barplot(x="Built_Remod",y="count" ,data=train_df_remodeling_info)

"""Many houses are not remodelled.
Two feature engineering are required
- Addition of the variable whether the house has been remodeled
- Removal of the variable YearRemod
"""

# Check Built_Remod Feature
fig,ax = plt.subplots(nrows=1, ncols=1,constrained_layout=True,figsize=(10,8))
fig.suptitle("Between Built year and Remodeling year as feature and check linearity")
bdf=train_df.toPandas()
# bdf=bdf.loc[bdf["Built_Remod"]!=0]

sns.regplot(x="Built_Remod", y="SalePrice", data=train_df.toPandas(), ax=ax)

bdf["Built_Remod"].value_counts()

"""It does not seem to affect SalePrice of the house directly."""

train_df.filter(train_df.YearBuilt.isin([1880,1892,1893])).toPandas()[['Built_Remod','SalePrice']].sort_values('SalePrice')

train_df.toPandas()[['Built_Remod','SalePrice']].sort_values('SalePrice')

# SalePrice with year between built and remodeling
plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=plt.figaspect(0.25))
fig.suptitle("SalePrice with year between built and remodeling")

sale_price_byBuiltRemod = train_df.toPandas()[['Built_Remod','SalePrice']].sort_values('SalePrice')
sns.lineplot(y="SalePrice",x="Built_Remod",data=sale_price_byBuiltRemod, ax=ax)

"""There is a clear observation that when the year of remodeling is more than 60 years after the year of construction, the SalePrice increase a lot. Based on this, it can be estimated that the SalePrice will increase a lot when the old houses are remodeled."""

# Some abnormal values are removed
cond = (col('YearBuilt') <= 1900) & (col('SalePrice') > 200000)

print("After remove abnormal")
print(train_df.filter(~cond).count())
train_df.filter(~cond).toPandas()

train_df_preprocess = train_df.filter(~cond)

# Avg SalePrice by Built Year after remove abnormal
plt.figure(figsize=(18,12))
fig,ax = plt.subplots(nrows=3,ncols=2,figsize=plt.figaspect(0.25))
fig.suptitle("Avg SalePrice with Built year after remove abnormal")

sale_price_byYear = train_df_preprocess.filter(train_df_preprocess.YrSold == 2006).groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax[0][0])

sale_price_byYear = train_df_preprocess.filter(train_df_preprocess.YrSold == 2007).groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax[0][1])

sale_price_byYear = train_df_preprocess.filter(train_df_preprocess.YrSold == 2008).groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax[1][0])

sale_price_byYear = train_df_preprocess.filter(train_df_preprocess.YrSold == 2009).groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax[1][1])

sale_price_byYear = train_df_preprocess.filter(train_df_preprocess.YrSold == 2010).groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax[2][0])

plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=plt.figaspect(0.25))

sale_price_byYear = train_df_preprocess.groupby("YearBuilt").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YearBuilt",data=sale_price_byYear, ax=ax)

"""The new houses seem to have larger SalePrice"""

# Mean SalePrice by Year
plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=plt.figaspect(0.25))

sale_price_byYear = train_df_preprocess.groupby("YrSold").agg(mean("SalePrice")).toPandas()
sns.lineplot(y="avg(SalePrice)",x="YrSold",data=sale_price_byYear, ax=ax)

"""It is estimated that some high price houses are sold in particular years"""

plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=2,figsize=plt.figaspect(0.25))

# Max SalePrice by Year
sale_price_byYear = train_df_preprocess.groupby("YrSold").agg(max("SalePrice")).toPandas()
sns.lineplot(y="max(SalePrice)",x="YrSold",data=sale_price_byYear, ax=ax[0])
ax[0].set_title("Max SalePrice by Year")

# Min SalePrice by Year
sale_price_byYear = train_df_preprocess.groupby("YrSold").agg(min("SalePrice")).toPandas()
sns.lineplot(y="min(SalePrice)",x="YrSold",data=sale_price_byYear, ax=ax[1])
ax[1].set_title("Min SalePrice by Year")

"""Max SalePrice seem to have large impact"""

train_df.toPandas().boxplot(column=['SalePrice', 'YrSold'],by=['YrSold'])
plt.tight_layout()

"""There are some houses with exceptionally high SalePrice.

---

**Normal Price House VS High Price House**
"""

Q3 = train_df.select("SalePrice").toPandas().quantile(.75)
Q1 = train_df.select("SalePrice").toPandas().quantile(.25)
IQR = Q3 - Q1

# divide outlier data and normal data
# compare A group(high price house) vs B group(normal price house)
train_pd = train_df.toPandas()
outlier_condition = Q3 + (1.5*IQR)
high_value_house = train_pd[train_pd["SalePrice"] > outlier_condition[0]]
normal_value_house = train_pd[train_pd["SalePrice"] <= outlier_condition[0]]

high_value_house.dropna(inplace=True)
normal_value_house.dropna(inplace=True)

high_value_house.head()

normal_value_house.head()

# Group compare (A vs B)
# 'OverallCond' feature has no any different
print("High Price")
print(high_value_house[high_value_house["LotFrontage"]!="NA"][['LotFrontage','LotArea','OverallQual','LowQualFinSF','TotRmsAbvGrd','Fireplaces','GarageArea',
                                                       'WoodDeckSF','ScreenPorch','SalePrice']].astype(int).describe())

print("-"*70)

print("Normal Price")
print(normal_value_house[normal_value_house["LotFrontage"]!="NA"][['LotFrontage','LotArea','OverallQual','LowQualFinSF','TotRmsAbvGrd','Fireplaces','GarageArea',
                                                       'WoodDeckSF','ScreenPorch','SalePrice']].astype(int).describe())

"""For variable LotArea and WoodDeckSF, they tend to have higher max value and average in high-price houses, which can be regarded as the major variable. For other variables, the average value is higher and the max value is relatively lower."""

plt.figure(figsize=(15,10))
fig,ax = plt.subplots(nrows=1,ncols=1,figsize=plt.figaspect(0.45))

sale_condition = train_df.groupby("SaleCondition").count().toPandas()
ax.pie(x="count",labels="SaleCondition",data=sale_condition, autopct='%1.1f%%', shadow=True,startangle=90
    , radius=1)

print("High price house sale condition")
print(high_value_house.groupby("SaleCondition").count()["SalePrice"])
print("-"*50)
print("low price house sale condition")
print(normal_value_house.groupby("SaleCondition").count()["SalePrice"])

"""**Total linear relationship**"""

columnList = train_df.drop(*cols).columns
fig,ax = plt.subplots(nrows=8,ncols=5,figsize=(22,20),constrained_layout=True)
fig.subplots_adjust(hspace=2)

idx, jdx = 0, 0
for arg in columnList:
    sns.regplot(x=arg,y="SalePrice",data=train_df.toPandas(),ax=ax[jdx][idx])
    ax[jdx,idx].set_title(arg)
    if idx == 4:
        jdx += 1
        idx = 0
    else:
        idx +=1

"""Some variable like LotArea, BsmtFinSF, TotalBsmfF has linear relationship with SalePrice

### Categorical variable Targeting
"""

print("Pandas Style")
print(train_df.toPandas()["Street"].value_counts())

print("-"*10)

# Analyze Spark DataFrame
print("Spark Style")
print(train_df.groupBy('Street').count().show())

print(train_df.groupBy("HouseStyle").count().show())

print(train_df.groupBy("SaleType").count().show())

train_df.groupBy("SaleType").count().toPandas()

categorical_df = train_df.groupBy("SaleType").count().toPandas()
sns.barplot(x="SaleType",y="count",data=categorical_df)

columnList = [item[0] for item in train_df.dtypes if item[1].startswith('string')]
len(columnList)

columnList = [item[0] for item in train_df.dtypes if item[1].startswith('string')]

fig,ax = plt.subplots(nrows=9,ncols=5,figsize=(22,20),constrained_layout=True)
fig.subplots_adjust(hspace=2)

idx, jdx = 0, 0
for arg in columnList:
    categorical_df = train_df.groupBy(arg).count().toPandas()
    sns.barplot(x=arg,y="count",data=categorical_df,ax=ax[jdx][idx])
    ax[jdx,idx].set_title(arg)
    if idx == 4:
        jdx += 1
        idx = 0
    else:
        idx +=1

"""Variable street, PoolQC, Heating, BsmtFinType2, Condition2,RoofMati,Utilites, and Alley seem to have less influence"""

# Remove Feature which is indiscriminate from categorical variable
#cat_minus_col = ['Street', 'PoolQC', 'Heating', 'BsmtFinType2', 'Condition2','RoofMati','Utilites', 'Alley']
cat_plus_col = ['LotFrontage','LotArea','OverallQual','MasVnrArea','BsmtFinSF1','TotalBsmtSF','1stFirSF','GrLivArea','TotRmsAbvGrd','GarageYrBlt','GarageArea','WoodDeckSF','OpenPorchSF','2ndFirSF','YearBuilt','YearRemodAdd']
#train_cat_col = [x for x in columnList if x not in cat_minus_col]
train_cat_col = [x for x in columnList if x in cat_plus_col]
cat_minus_col = [x for x in columnList if x not in cat_plus_col]
#train_cat_col[:3]

train_df = train_df.drop(*cat_minus_col)

train_cat_col_ = [x+"_" for x in train_cat_col]

"""# Summary
- Variable 'LotArea' and 'WoodDeckSF' seem to have high correlation with SalePrice

- 'Street', 'PoolQC', 'Heating', 'BsmtFinType2', 'Condition2','RoofMati','Utilites', 'Alley' can be excluded from the training model. The other variables can be used for training

- The new houses (after Year 2000) tend to have higher SalePrice, but old houses (before Year 1900) that are remodeled with some additional condition also have high SalePrice

## **Predict**
"""

stringIndex = StringIndexer(inputCols=train_cat_col, 
                       outputCols=train_cat_col_)

stringIndex_model = stringIndex.fit(train_df)

train_df_ = stringIndex_model.transform(train_df).drop(*train_cat_col)
train_df_.toPandas().head(4)

train_df_ = train_df_.fillna(0)

vec_col = train_df_.drop("Id","SalePrice").columns
vec_asmbl = VectorAssembler(inputCols=vec_col, 
                           outputCol='features')

train_df_ = vec_asmbl.transform(train_df_).select('features', 'SalePrice')
train_df_.show(4, truncate=False)

train_data, test_data = train_df_.randomSplit([.8,.2], seed=42)

lr = LinearRegression(labelCol='SalePrice', featuresCol = 'features', predictionCol = 'resultPrice', 
                      maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False)

model = lr.fit(train_data)

predictions = model.transform(test_data)

predict_result = predictions.select("resultPrice", "SalePrice")

predict_result.show()

predict_result = predict_result.withColumn("resultPrice",predict_result.resultPrice.cast("int"))
predict_result.show()

result_df = predict_result.toPandas()
mean_absolute_error(result_df.resultPrice, result_df.SalePrice)

print("MAE: {0}".format(model.summary.meanAbsoluteError))

from pyspark.ml.evaluation import RegressionEvaluator
lr_evaluator = RegressionEvaluator(predictionCol="resultPrice", labelCol="SalePrice",metricName="r2")
print("R Squared (R2) on val data = %g" % lr_evaluator.evaluate(result_df))

